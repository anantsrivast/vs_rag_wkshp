{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOoHdxNqHSJYCBPDLkoQVbI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anantsrivast/vs_rag_wkshp/blob/main/vector_search_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AmTNhkaf8wY"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/anantsrivast/vs_rag_wkshp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "get_ipython().run_cell_magic('javascript', '', 'IPython.notebook.clear_all_output();')\n"
      ],
      "metadata": {
        "id": "BAq0eXFmLrMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import Widget\n",
        "Widget.close_all()\n"
      ],
      "metadata": {
        "id": "_MLzj9L3K-jg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -r \"/content/vs_rag_wkshp/requirements.txt\" --upgrade --no-cache-dir"
      ],
      "metadata": {
        "id": "4GNx4lfZgCyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pymongo import MongoClient"
      ],
      "metadata": {
        "id": "SXYO4DXPuHLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retain the quotes (\"\") when pasting the URI\n",
        "from google.colab import userdata\n",
        "MONGODB_URI = userdata.get('mongo_uri')\n",
        "# Initialize a MongoDB Python client\n",
        "mongodb_client = MongoClient(MONGODB_URI, appname=\"devrel.workshop.rag\")\n",
        "# Check the connection to the server\n",
        "mongodb_client.admin.command(\"ping\")"
      ],
      "metadata": {
        "id": "uDgcfG63vb2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You may see a warning upon running this cell. You can ignore it.\n",
        "import pandas as pd\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "aAIh8woFvqYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the `mongodb-docs` dataset from Hugging Face\n",
        "data = load_dataset(\"mongodb/mongodb-docs\", split=\"train\")\n",
        "# Convert the dataset into a dataframe first, then into a list of Python objects/dictionaries\n",
        "docs = pd.DataFrame(data).to_dict(\"records\")"
      ],
      "metadata": {
        "id": "gYk3V1JRxwnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note the number of documents in the dataset\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "ibRCfO7VzrKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview a document to understand its structure\n",
        "docs[0]"
      ],
      "metadata": {
        "id": "qsf9ulsUztKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from typing import Dict, List"
      ],
      "metadata": {
        "id": "fFjrYqQ1zu3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common list of separators for text data\n",
        "separators = [\"\\n\\n\", \"\\n\", \" \", \"\", \"#\", \"##\", \"###\"]"
      ],
      "metadata": {
        "id": "HNjObhhZz4DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the `RecursiveCharacterTextSplitter` from LangChain to first split a piece of text on the list of `separators` above.\n",
        "# Then recursively merge them into tokens until the specified chunk size is reached.\n",
        "# For text data, you typically want to keep 1-2 paragraphs (~200 tokens) in a single chunk.\n",
        "# Chunk overlap of 15-20% of the chunk size is recommended to maintain context between chunks.\n",
        "# The `model_name` parameter indicates which encoder to use for tokenization, in this case GPT-4's encoder.\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    model_name=\"gpt-4\", separators=separators, chunk_size=200, chunk_overlap=30\n",
        ")"
      ],
      "metadata": {
        "id": "MEqyPmIRz6Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chunks(doc: Dict, text_field: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Chunk up a document.\n",
        "\n",
        "    Args:\n",
        "        doc (Dict): Parent document to generate chunks from.\n",
        "        text_field (str): Text field to chunk.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: List of chunked documents.\n",
        "    \"\"\"\n",
        "    # Extract the field to chunk from `doc`\n",
        "    text = doc[text_field]\n",
        "    # Split `text` using the appropriate method of the `RecursiveCharacterTextSplitter` class\n",
        "    # NOTE: `text` is a string\n",
        "    chunks = text_splitter.split_text(text)\n",
        "\n",
        "    # Iterate through `chunks` and for each chunk:\n",
        "    # 1. Create a shallow copy of `doc`, call it `temp`\n",
        "    # 2. Set the `text_field` field in `temp` to the content of the chunk\n",
        "    # 3. Append `temp` to `chunked_data`\n",
        "    chunked_data = []\n",
        "    for chunk in chunks:\n",
        "        temp = doc.copy()\n",
        "        temp[text_field]=chunk\n",
        "        chunked_data.append(temp)\n",
        "\n",
        "    return chunked_data"
      ],
      "metadata": {
        "id": "FKG2hemMz9Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_docs = []\n",
        "# Iterate through `docs`, use the `get_chunks` function to chunk up the \"body\" field in the documents, and add the list of chunked documents to `split_docs` initialized above.\n",
        "for doc in docs:\n",
        "    chunks = get_chunks(doc,\"body\")\n",
        "    split_docs.extend(chunks)"
      ],
      "metadata": {
        "id": "wHj5jVtF0CiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice that the length of `split_docs` is greater than the length of `docs` from Step 2 above\n",
        "# This is because each document in `docs` has been split into multiple chunks\n",
        "len(split_docs)"
      ],
      "metadata": {
        "id": "lGm5LMdQ0MCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview a chunked document to understand its structure\n",
        "# Note that the structure looks similar to the original docs, except the `body` field now contains smaller chunks of text\n",
        "split_docs[0]"
      ],
      "metadata": {
        "id": "BfJBZ0Ys0N_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the `gte-small` model using the Sentence Transformers library\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "embedding_model = SentenceTransformer(\"thenlper/gte-small\")"
      ],
      "metadata": {
        "id": "jrvnuOwZ0SK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that takes a piece of text (`text`) as input, embeds it using the `embedding_model` instantiated above and returns the embedding as a list\n",
        "# An array can be converted to a list using the `tolist()` method\n",
        "def get_embedding(text: str) -> List[float]:\n",
        "    \"\"\"\n",
        "    Generate the embedding for a piece of text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to embed.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: Embedding of the text as a list.\n",
        "    \"\"\"\n",
        "    embedding = embedding_model.encode(text)\n",
        "    return embedding.tolist()"
      ],
      "metadata": {
        "id": "DbyPsnyc0_uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_docs = []\n",
        "# Add an `embedding` field to each dictionary in `split_docs`\n",
        "# The `embedding` field should correspond to the embedding of the value of the `body` field\n",
        "# Use the `get_embedding` function defined above to generate the embedding\n",
        "# Append the updated dictionaries to `embedded_docs` initialized above.\n",
        "for doc in tqdm(split_docs):\n",
        "    doc[\"embedding\"]= get_embedding(doc[\"body\"])\n",
        "    embedded_docs.append(doc)"
      ],
      "metadata": {
        "id": "2Tu9nFwp1OKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that the length of `embedded_docs` is the same as that of `split_docs`\n",
        "len(embedded_docs)"
      ],
      "metadata": {
        "id": "5zQN1bT21Q_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_unique_db_name(prefix=\"mongodb_genai_devday_rag\"):\n",
        "    \"\"\"Generate a unique database name with timestamp and UUID\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    unique_id = str(uuid.uuid4())[:8]  # Use first 8 characters of UUID\n",
        "    return f\"{prefix}_{timestamp}_{unique_id}\"\n"
      ],
      "metadata": {
        "id": "ZMI2IUzp1V5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Name of the database -- Change if needed or leave as is\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "DB_NAME = generate_unique_db_name()\n",
        "# Name of the collection -- Change if needed or leave as is\n",
        "COLLECTION_NAME = \"knowledge_base\"\n",
        "# Name of the vector search index -- Change if needed or leave as is\n",
        "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\""
      ],
      "metadata": {
        "id": "-rfSqJxB1UEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = mongodb_client[DB_NAME]"
      ],
      "metadata": {
        "id": "7HvFfHAg5f-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the `COLLECTION_NAME` collection.\n",
        "# Use the `db` and collection name defined above.\n",
        "collection = db[COLLECTION_NAME]"
      ],
      "metadata": {
        "id": "2HNFMwcH637k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bulk delete all existing records from the collection defined above\n",
        "collection.delete_many({})"
      ],
      "metadata": {
        "id": "3CIXnPwc67cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bulk insert `embedded_docs` into the collection defined above -- should be a one-liner\n",
        "collection.insert_many(embedded_docs)\n",
        "\n",
        "print(f\"Ingested {collection.count_documents({})} documents into the {COLLECTION_NAME} collection.\")"
      ],
      "metadata": {
        "id": "ICEUwD4D69zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vector index definition specifying:\n",
        "# path: Path to the embeddings field\n",
        "# numDimensions: Number of embedding dimensions- depends on the embedding model used\n",
        "# similarity: Similarity metric. One of cosine, euclidean, dotProduct.\n",
        "model = {\n",
        "    \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
        "    \"type\": \"vectorSearch\",\n",
        "    \"definition\": {\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"type\": \"vector\",\n",
        "                \"path\": \"embedding\",\n",
        "                \"numDimensions\": 384,\n",
        "                \"similarity\": \"cosine\",\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "uB8Bvj4w6_-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vector search index with the above definition for the `collection` collection\n",
        "collection.create_search_index(model)"
      ],
      "metadata": {
        "id": "WX2QxNAoK5ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to retrieve relevant documents for a user query using vector search\n",
        "def vector_search(user_query: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Retrieve relevant documents for a user query using vector search.\n",
        "\n",
        "    Args:\n",
        "    user_query (str): The user's query string.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of matching documents.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate embedding for the `user_query` using the `get_embedding` function defined in Step 4\n",
        "    query_embedding = get_embedding(user_query)\n",
        "\n",
        "    # Define an aggregation pipeline consisting of a $vectorSearch stage, followed by a $project stage\n",
        "    # Set the number of candidates to 150 and only return the top 5 documents from the vector search\n",
        "    # In the $project stage, exclude the `_id` field and include only the `body` field and `vectorSearchScore`\n",
        "    # NOTE: Use variables defined previously for the `index`, `queryVector` and `path` fields in the $vectorSearch stage\n",
        "    pipeline = [{\"$vectorSearch\" : {\"queryVector\": query_embedding,\n",
        "            \"path\": \"embedding\",\n",
        "            \"numCandidates\": 100,  # controls the search scope\n",
        "            \"limit\": 5,            # top K results\n",
        "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME}}]\n",
        "\n",
        "    # Execute the aggregation `pipeline` and store the results in `results`\n",
        "    results = collection.aggregate(pipeline)\n",
        "    return list(results)"
      ],
      "metadata": {
        "id": "js8B44hYInng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_search(\"What are some best practices for data backups in MongoDB?\")"
      ],
      "metadata": {
        "id": "QEgk55RxLNV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O phi-2.gguf https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "id": "9YnsU8hWWoBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain-ollama\n",
        "\n",
        "from llama_cpp import Llama\n",
        "\n",
        "\n",
        "llm = Llama(model_path=\"phi-2.gguf\", n_ctx=1024)"
      ],
      "metadata": {
        "id": "6IUSGJ1TrT8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to create the user prompt for our RAG application\n",
        "def create_prompt(user_query: str) -> str:\n",
        "    \"\"\"\n",
        "    Create a chat prompt that includes the user query and retrieved context.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's query string.\n",
        "\n",
        "    Returns:\n",
        "        str: The chat prompt string.\n",
        "    \"\"\"\n",
        "    # Retrieve the most relevant documents for the `user_query` using the `vector_search` function defined in Step 7\n",
        "    context = vector_search(user_query)\n",
        "    # Join the retrieved documents into a single string, where each document is separated by two new lines (\"\\n\\n\")\n",
        "    context = \"\\n\\n\".join([doc.get('body') for doc in context])\n",
        "    # Prompt consisting of the question and relevant context to answer it\n",
        "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "YBIa-jbCsu2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_chat_prompt(messages):\n",
        "    \"\"\"Formats a list of role/content messages into a prompt string\"\"\"\n",
        "    prompt = \"\"\n",
        "    for msg in messages:\n",
        "        if msg[\"role\"] == \"system\":\n",
        "            prompt += f\"<|system|>\\n{msg['content']}\\n\"\n",
        "        elif msg[\"role\"] == \"user\":\n",
        "            prompt += f\"<|user|>\\n{msg['content']}\\n\"\n",
        "        elif msg[\"role\"] == \"assistant\":\n",
        "            prompt += f\"<|assistant|>\\n{msg['content']}\\n\"\n",
        "    prompt += \"<|assistant|>\\n\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "RZ-MIpaLXKkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to answer user queries\n",
        "def generate_answer(user_query: str) -> None:\n",
        "    \"\"\"\n",
        "    Generate an answer to the user query.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's query string.\n",
        "    \"\"\"\n",
        "    # Use the `create_prompt` function above to create a chat prompt\n",
        "    prompt = create_prompt(user_query)\n",
        "    # Format the message to the LLM in the format {\"role\": <role_value>, \"content\": <content_value>}\n",
        "    # The role value for user messages must be \"user\"\n",
        "    # Use the `prompt` created above to populate the `content` field in the chat message\n",
        "    messages = [{\"role\": \"system\", \"content\": prompt}]\n",
        "    # Send the chat messages to LLM\n",
        "    prompt=format_chat_prompt(messages)\n",
        "    #print(prompt)\n",
        "    response = llm(prompt, max_tokens=500)\n",
        "    # Print the response\n",
        "    #response = requests.post(url, json=messages)\n",
        "    print(response[\"choices\"][0][\"text\"])\n",
        "    #print(response.json()['response'])\n",
        "    # print(response.json()[\"text\"])"
      ],
      "metadata": {
        "id": "u5r8l2CQsxvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_answer(\"What are some best practices for data backups in MongoDB?\")"
      ],
      "metadata": {
        "id": "R1IMYkiNs2Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cxSKwSmZkomL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}